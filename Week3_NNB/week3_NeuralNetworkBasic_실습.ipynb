{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Neural Network Basic - Week3 과제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "- MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "X_test shape: (10000, 784)\n",
      "Y_train shape: (60000,)\n",
      "Y_train shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "print(f'Y_train shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function \n",
    "- sigmoid & relu : hidden layer activation function \n",
    "- softmax : output layer activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    # sigmoid 함수를 작성하세요 \n",
    "    def forward(x):\n",
    "        #sigy = []\n",
    "        #for i in x:\n",
    "        sig=1/(1+math.exp(-x))  \n",
    "            #sigy.append(sig)    \n",
    "        return np.array(sigy)\n",
    "    \n",
    "    # sigmoid 함수의 미분을 작성하세요\n",
    "    def backward(x):\n",
    "        #sigdify = []\n",
    "        #for i in x:\n",
    "        sigdif=math.exp(-x)/(1+math.exp(-x))**2\n",
    "        #sigdify.append(sigdif)\n",
    "        return sigdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    # relu 함수를 작성하세요\n",
    "    def forward(x):\n",
    "        if x<=0: y=0\n",
    "        else: y=x\n",
    "        return y\n",
    "    \n",
    "    # relu 함수의 미분을 작성하세요\n",
    "    def backward(x):\n",
    "        if x<=0: reludif=0\n",
    "        else: reludif=1\n",
    "        return reludif "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def forward(z):\n",
    "        y = []\n",
    "        for zi in z:\n",
    "            c = np.max(zi)\n",
    "            exp_zi = np.exp(zi-c)\n",
    "            sum_exp_zi = np.sum(exp_zi)\n",
    "            yi = exp_zi / sum_exp_zi\n",
    "            y.append(yi)\n",
    "\n",
    "        return np.array(y)\n",
    "    \n",
    "    def backward(p, y) :\n",
    "        dp = p.copy()\n",
    "        for dpi, yi in zip(dp, y):\n",
    "            for k in range(dp.shape[1]):\n",
    "                if k == yi :\n",
    "                    dpi[k] -= 1\n",
    "        return dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, y):\n",
    "    loss = []\n",
    "    for pi, yi in zip(p, y):\n",
    "        for k in range(p.shape[1]):\n",
    "            if k == yi:\n",
    "                loss.append((-1) * (np.log(pi[k] + 1e-8))) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, std=1e-4) :\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        self.weight = np.random.randn(input_size, output_size)*std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "- 각 메소드와 변수들의 역할을 주석으로 달아주세요! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet:\n",
    "    # CustomNet을 선언할 때 생성되는 값들입니다.\n",
    "    def __init__(self, lr=0.0001, epoch=500, batch_size=200):\n",
    "        self.lr = lr #learning rate.학습률\n",
    "        self.epoch = epoch  #학습반복횟수 (500으로 설정해놓음)\n",
    "        self.batch_size = batch_size #배치사이즈 (200으로 설정해놓음)\n",
    "        self.loss_function = cross_entropy #손실함수는 크로스 엔트로피로 설정\n",
    "        self.layers = []\n",
    "        self.activations = [softmax] #활성화 함수는 softmax로 설정\n",
    "        self.nodes = []\n",
    "    \n",
    "    # Layer를 추가할 때 호출합니다\n",
    "    def addLayer(self, Layer): \n",
    "        self.layers.append(Layer) \n",
    "        if not self.nodes: \n",
    "            self.nodes.append(np.zeros(Layer.input_size))#노드를 0으로 채워줍니다.\n",
    "        self.nodes.append(np.zeros(Layer.output_size))\n",
    "        \n",
    "    # Activation Function을 추가할 때 호출합니다\n",
    "    def addActivation(self, Activation):\n",
    "        tmp = self.activations.pop() \n",
    "        self.activations.append(Activation) \n",
    "        self.activations.append(tmp) \n",
    "        \n",
    "    # 순전파 함수\n",
    "    def _forward(self, X):\n",
    "        self.nodes[0] = X.copy() \n",
    "        output = X.copy() \n",
    "        for i in range(len(self.layers)): \n",
    "            Layer = self.layers[i] \n",
    "            Activation = self.activations[i] \n",
    "            output = np.dot(self.nodes[i], Layer.weight) \n",
    "            output = output+ Layer.bias \n",
    "            output = Activation.forward(output) \n",
    "            self.nodes[i+1] = output \n",
    "        return output   \n",
    "    \n",
    "    # 역전파 함수\n",
    "    def _backward(self, X, output, y) :\n",
    "        for i in reversed(range(len(self.layers))): \n",
    "            a = self.nodes[i+1] \n",
    "            Layer = self.layers[i] \n",
    "            Activation = self.activations[i] \n",
    "            \n",
    "            if i+1 == len(self.layers): \n",
    "                error = Activation.backward(output, y)\n",
    "            else:\n",
    "                error *= Activation.backward(a)\n",
    "            Layer.weight -= np.dot(error.T, self.nodes[i]).T*self.lr/X.shape[0] \n",
    "            Layer.bias -= error.sum(axis=0)*self.lr/X.shape[0]\n",
    "            error = np.dot(error, Layer.weight.T) \n",
    "            \n",
    "    # Accrucy를 반환합니다\n",
    "    def _accuracy(self, output, y):\n",
    "        pre_p = np.argmax(output, axis=1)\n",
    "        return np.sum(pre_p==y)/y.shape[0] \n",
    "    \n",
    "    # 데이터셋에 모델을 fit할때 호출합니다\n",
    "    def fit(self, X, y, val_X, val_y):\n",
    "        history = {'val_acc': [],'val_loss': []}\n",
    "        N = X.shape[0]\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(N//self.batch_size): \n",
    "                batch_mask = np.random.choice(N, self.batch_size)\n",
    "                X_batch = X[batch_mask] \n",
    "                y_batch = y[batch_mask] \n",
    "                output = self._forward(X_batch) \n",
    "                self._backward(X_batch, output, y_batch)\n",
    "            \n",
    "            #accuracy와 loss를 기록해둡시다\n",
    "            output = self._forward(val_X) \n",
    "            history[\"val_acc\"].append(self._accuracy(output, val_y)) \n",
    "            history[\"val_loss\"].append(sum(self.loss_function(output, val_y))) \n",
    "            \n",
    "            #중간중간 기록을 찍어볼 때 사용. 적절히 조절해 쓰세요\n",
    "            if i % 10 == 0:\n",
    "                print(i, \"test accuracy :\", history[\"val_acc\"][-1])\n",
    "                print(i, \"test loss     :\", history[\"val_loss\"][-1])\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing\n",
    "- Network parameter, Layer architecture, Activation function .. 등등 다양한 하이퍼파라미터를 커스터마이징하여 높은 성능에 도달해 봅시다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터를 적절히 조절해 뉴럴넷을 선언하세요\n",
    "nn = CustomNet(lr=0.005, epoch=200, batch_size=400)\n",
    "\n",
    "# 원하는 만큼 층과 활성화 함수를 쌓아 주세요. 기본적으로 2Layer를 예시로 적어드립니다\n",
    "nn.addLayer(Layer(784,100))\n",
    "nn.addActivation(sigmoid)\n",
    "nn.addLayer(Layer(100,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,)\n",
      "(100, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# 선언한 뉴럴넷의 구조입니다\n",
    "for layer in nn.layers:\n",
    "    print(layer.weight.shape, layer.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-122851951a08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-37faafe88671>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, val_X, val_y)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-37faafe88671>\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-a278da5640bc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m#sigy = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#for i in x:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;31m#sigy.append(sig)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Loss Visualization\n",
    "- 자유롭게 Accuracy나 Loss를 시각화하여 확인하고 결과를 확인해 보세요! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
